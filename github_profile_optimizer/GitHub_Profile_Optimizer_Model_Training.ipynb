{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e65022",
   "metadata": {},
   "source": [
    "# GitHub Profile Optimizer - Model Training\n",
    "\n",
    "This notebook trains and fine-tunes ML models to predict optimization recommendations for GitHub profiles based on historical data and successful profile patterns.\n",
    "\n",
    "## Features\n",
    "- Generate synthetic training data\n",
    "- Train classification models for different recommendation types\n",
    "- Train regression model for impact prediction\n",
    "- Hyperparameter tuning capabilities\n",
    "- Model evaluation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e651020",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for model training.\"\"\"\n",
    "    n_samples: int = 2000\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    cv_folds: int = 5\n",
    "    tune_hyperparameters: bool = True\n",
    "    model_type: str = 'random_forest'  # 'random_forest' or 'gradient_boost'\n",
    "    output_dir: str = 'models'\n",
    "    data_path: Optional[str] = None\n",
    "    \n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate configuration parameters.\"\"\"\n",
    "        if self.n_samples < 100:\n",
    "            raise ValueError(\"n_samples must be at least 100\")\n",
    "        if not 0 < self.test_size < 1:\n",
    "            raise ValueError(\"test_size must be between 0 and 1\")\n",
    "        if self.cv_folds < 2:\n",
    "            raise ValueError(\"cv_folds must be at least 2\")\n",
    "        if self.model_type not in ['random_forest', 'gradient_boost']:\n",
    "            raise ValueError(\"model_type must be 'random_forest' or 'gradient_boost'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32367585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubProfileDataset:\n",
    "    \"\"\"Handles dataset creation and preprocessing for GitHub profiles.\"\"\"\n",
    "    \n",
    "    # Feature column definitions\n",
    "    FEATURE_COLUMNS = [\n",
    "        'profile_completeness',\n",
    "        'repo_count',\n",
    "        'total_stars',\n",
    "        'languages_count',\n",
    "        'recent_activity',\n",
    "        'description_quality',\n",
    "        'has_bio',\n",
    "        'has_location',\n",
    "        'has_company',\n",
    "        'follower_count',\n",
    "        'following_count',\n",
    "        'avg_stars_per_repo',\n",
    "        'follower_following_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Target column definitions\n",
    "    TARGET_COLUMNS = [\n",
    "        'needs_bio',\n",
    "        'needs_repo_descriptions',\n",
    "        'needs_activity_boost',\n",
    "        'needs_language_showcase',\n",
    "        'needs_pin_repos'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"Initialize dataset handler.\n",
    "        \n",
    "        Args:\n",
    "            config: Training configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.scaler: Optional[StandardScaler] = None\n",
    "        \n",
    "    def generate_synthetic_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate synthetic GitHub profile data for training.\n",
    "        \n",
    "        This generates realistic profiles with correlated features and targets\n",
    "        based on common GitHub profile patterns.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with synthetic profile data\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If n_samples is invalid\n",
    "        \"\"\"\n",
    "        n_samples = self.config.n_samples\n",
    "        if n_samples < 1:\n",
    "            raise ValueError(f\"Invalid n_samples: {n_samples}\")\n",
    "            \n",
    "        logger.info(f\"Generating {n_samples} synthetic GitHub profiles...\")\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        np.random.seed(self.config.random_state)\n",
    "        \n",
    "        # Generate base features with realistic distributions\n",
    "        # Beta distribution for percentages (0-1)\n",
    "        # Poisson for counts\n",
    "        # Exponential for long-tail distributions (stars, followers)\n",
    "        \n",
    "        data = {\n",
    "            # Profile completeness: beta(5,2) gives mean ~0.71\n",
    "            'profile_completeness': np.clip(np.random.beta(5, 2, n_samples), 0, 1),\n",
    "            \n",
    "            # Repository count: Poisson mean=15\n",
    "            'repo_count': np.clip(np.random.poisson(15, n_samples), 0, 200),\n",
    "            \n",
    "            # Total stars: Exponential scale=50, right-skewed\n",
    "            'total_stars': np.clip(np.random.exponential(50, n_samples).astype(int), 0, 10000),\n",
    "            \n",
    "            # Languages: Most devs use 1-7 languages\n",
    "            'languages_count': np.clip(np.random.randint(1, 8, n_samples), 1, 20),\n",
    "            \n",
    "            # Recent activity: beta(3,2) gives mean ~0.6\n",
    "            'recent_activity': np.clip(np.random.beta(3, 2, n_samples), 0, 1),\n",
    "            \n",
    "            # Description quality: beta(4,2) gives mean ~0.67\n",
    "            'description_quality': np.clip(np.random.beta(4, 2, n_samples), 0, 1),\n",
    "            \n",
    "            # Binary features: has_bio (70% have), has_location (75%), has_company (60%)\n",
    "            'has_bio': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "            'has_location': np.random.choice([0, 1], n_samples, p=[0.25, 0.75]),\n",
    "            'has_company': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
    "            \n",
    "            # Follower/following counts: exponential distributions\n",
    "            'follower_count': np.clip(np.random.exponential(100, n_samples).astype(int), 0, 50000),\n",
    "            'following_count': np.clip(np.random.exponential(50, n_samples).astype(int), 0, 5000),\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Calculate derived features\n",
    "        df['avg_stars_per_repo'] = df['total_stars'] / (df['repo_count'] + 1)  # +1 to avoid division by zero\n",
    "        df['follower_following_ratio'] = df['follower_count'] / (df['following_count'] + 1)\n",
    "        \n",
    "        # Generate success score (0-1 scale) - weighted combination of features\n",
    "        df['profile_success_score'] = self._calculate_success_score(df)\n",
    "        \n",
    "        # Generate recommendation targets based on business logic\n",
    "        df = self._generate_targets(df)\n",
    "        \n",
    "        logger.info(f\"Generated dataset shape: {df.shape}\")\n",
    "        logger.info(f\"Feature columns: {len(self.FEATURE_COLUMNS)}\")\n",
    "        logger.info(f\"Target columns: {len(self.TARGET_COLUMNS)}\")\n",
    "        \n",
    "        # Log target distribution\n",
    "        for target in self.TARGET_COLUMNS:\n",
    "            positive_rate = df[target].mean()\n",
    "            logger.info(f\"  {target}: {positive_rate:.2%} positive samples\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _calculate_success_score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate profile success score based on multiple factors.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame with features\n",
    "            \n",
    "        Returns:\n",
    "            Series with success scores (0-1)\n",
    "        \"\"\"\n",
    "        # Weighted combination of normalized features\n",
    "        score = (\n",
    "            0.25 * df['profile_completeness'] +\n",
    "            0.20 * np.clip(np.log1p(df['total_stars']) / 10, 0, 1) +\n",
    "            0.20 * df['recent_activity'] +\n",
    "            0.15 * df['description_quality'] +\n",
    "            0.20 * np.clip(np.log1p(df['follower_count']) / 10, 0, 1)\n",
    "        )\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        return np.clip(score, 0, 1)\n",
    "    \n",
    "    def _generate_targets(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate target labels based on business rules.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame with features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with added target columns\n",
    "        \"\"\"\n",
    "        # needs_bio: Missing bio AND low profile completeness\n",
    "        df['needs_bio'] = (\n",
    "            (df['has_bio'] == 0) & \n",
    "            (df['profile_completeness'] < 0.6)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # needs_repo_descriptions: Low description quality\n",
    "        df['needs_repo_descriptions'] = (\n",
    "            df['description_quality'] < 0.5\n",
    "        ).astype(int)\n",
    "        \n",
    "        # needs_activity_boost: Low recent activity\n",
    "        df['needs_activity_boost'] = (\n",
    "            df['recent_activity'] < 0.4\n",
    "        ).astype(int)\n",
    "        \n",
    "        # needs_language_showcase: High language diversity not showcased\n",
    "        df['needs_language_showcase'] = (\n",
    "            (df['languages_count'] >= 4) &\n",
    "            (df['profile_completeness'] < 0.7)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # needs_pin_repos: Popular repos exist but not highlighted\n",
    "        df['needs_pin_repos'] = (\n",
    "            (df['total_stars'] > 20) & \n",
    "            (df['repo_count'] > 5) &\n",
    "            (df['profile_completeness'] < 0.8)\n",
    "        ).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load_real_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load real GitHub profile data from JSON file.\n",
    "        \n",
    "        Expected JSON format:\n",
    "        [\n",
    "            {\n",
    "                \"profile_completeness\": 0.8,\n",
    "                \"repo_count\": 25,\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with real profile data\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If data file doesn't exist\n",
    "            ValueError: If data format is invalid\n",
    "        \"\"\"\n",
    "        data_path = self.config.data_path\n",
    "        \n",
    "        if not data_path:\n",
    "            logger.warning(\"No data path provided, generating synthetic data\")\n",
    "            return self.generate_synthetic_data()\n",
    "        \n",
    "        path = Path(data_path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading data from {data_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if not isinstance(data, list):\n",
    "                raise ValueError(\"JSON data must be a list of profile objects\")\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Validate required columns exist\n",
    "            missing_features = set(self.FEATURE_COLUMNS) - set(df.columns)\n",
    "            if missing_features:\n",
    "                raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "            \n",
    "            logger.info(f\"Loaded {len(df)} real profiles\")\n",
    "            return df\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "    \n",
    "    def preprocess_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, StandardScaler]:\n",
    "        \"\"\"Preprocess features for model training.\n",
    "        \n",
    "        Steps:\n",
    "        1. Select feature columns\n",
    "        2. Handle missing values (fill with 0)\n",
    "        3. Standardize features (mean=0, std=1)\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame with raw features\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (scaled features DataFrame, fitted scaler)\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If required feature columns are missing\n",
    "        \"\"\"\n",
    "        logger.info(\"Preprocessing features...\")\n",
    "        \n",
    "        # Validate feature columns exist\n",
    "        missing_cols = set(self.FEATURE_COLUMNS) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_cols}\")\n",
    "        \n",
    "        # Select and copy feature columns\n",
    "        X = df[self.FEATURE_COLUMNS].copy()\n",
    "        \n",
    "        # Log missing value statistics\n",
    "        missing_counts = X.isnull().sum()\n",
    "        if missing_counts.any():\n",
    "            logger.warning(\"Missing values detected:\")\n",
    "            for col, count in missing_counts[missing_counts > 0].items():\n",
    "                logger.warning(f\"  {col}: {count} missing ({count/len(X):.2%})\")\n",
    "        \n",
    "        # Fill missing values with 0 (conservative approach)\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # Fit and transform scaler\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        X_scaled_df = pd.DataFrame(\n",
    "            X_scaled, \n",
    "            columns=self.FEATURE_COLUMNS, \n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Log scaling statistics\n",
    "        logger.info(f\"Features scaled: {X_scaled_df.shape[1]} columns\")\n",
    "        logger.info(f\"Feature means: min={X_scaled_df.mean().min():.3f}, max={X_scaled_df.mean().max():.3f}\")\n",
    "        logger.info(f\"Feature stds: min={X_scaled_df.std().min():.3f}, max={X_scaled_df.std().max():.3f}\")\n",
    "        \n",
    "        self.scaler = scaler\n",
    "        return X_scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationModel:\n",
    "    \"\"\"Multi-output model for generating GitHub profile recommendations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"Initialize recommendation model.\n",
    "        \n",
    "        Args:\n",
    "            config: Training configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.scaler: Optional[StandardScaler] = None\n",
    "        self.feature_names: Optional[List[str]] = None\n",
    "        self.training_metrics: Dict[str, Dict[str, float]] = {}\n",
    "        \n",
    "    def _get_base_classifier(self):\n",
    "        \"\"\"Get base classifier based on configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Unfitted classifier instance\n",
    "        \"\"\"\n",
    "        if self.config.model_type == 'random_forest':\n",
    "            return RandomForestClassifier(\n",
    "                random_state=self.config.random_state,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'  # Handle imbalanced classes\n",
    "            )\n",
    "        elif self.config.model_type == 'gradient_boost':\n",
    "            return GradientBoostingClassifier(\n",
    "                random_state=self.config.random_state\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.config.model_type}\")\n",
    "    \n",
    "    def _get_param_grid(self) -> Dict[str, List]:\n",
    "        \"\"\"Get hyperparameter grid for tuning.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of hyperparameter ranges\n",
    "        \"\"\"\n",
    "        if self.config.model_type == 'random_forest':\n",
    "            return {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            }\n",
    "        else:  # gradient_boost\n",
    "            return {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            }\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Train classification models for each recommendation type.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame (already scaled)\n",
    "            y: Target DataFrame with recommendation columns\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with training metrics for each model\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If target columns are missing\n",
    "        \"\"\"\n",
    "        logger.info(f\"Training {self.config.model_type} models...\")\n",
    "        logger.info(f\"Training samples: {len(X)}\")\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        # Validate target columns\n",
    "        missing_targets = set(GitHubProfileDataset.TARGET_COLUMNS) - set(y.columns)\n",
    "        if missing_targets:\n",
    "            raise ValueError(f\"Missing target columns: {missing_targets}\")\n",
    "        \n",
    "        for rec_type in GitHubProfileDataset.TARGET_COLUMNS:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"Training model for: {rec_type}\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # Check class distribution\n",
    "            class_dist = y[rec_type].value_counts()\n",
    "            logger.info(f\"Class distribution:\\n{class_dist}\")\n",
    "            \n",
    "            if len(class_dist) < 2:\n",
    "                logger.warning(f\"Skipping {rec_type}: only one class present\")\n",
    "                continue\n",
    "            \n",
    "            # Split data with stratification to preserve class distribution\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y[rec_type],\n",
    "                    test_size=self.config.test_size,\n",
    "                    random_state=self.config.random_state,\n",
    "                    stratify=y[rec_type]\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                logger.error(f\"Failed to split data for {rec_type}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize base model\n",
    "            base_model = self._get_base_classifier()\n",
    "            \n",
    "            # Train with or without hyperparameter tuning\n",
    "            if self.config.tune_hyperparameters:\n",
    "                model = self._tune_model(base_model, X_train, y_train, rec_type)\n",
    "            else:\n",
    "                logger.info(\"Training with default parameters...\")\n",
    "                base_model.fit(X_train, y_train)\n",
    "                model = base_model\n",
    "            \n",
    "            # Evaluate model\n",
    "            metrics = self._evaluate_model(model, X_train, X_test, y_train, y_test, rec_type)\n",
    "            \n",
    "            # Store model and metrics\n",
    "            self.models[rec_type] = model\n",
    "            self.training_metrics[rec_type] = metrics\n",
    "            \n",
    "            # Log feature importance\n",
    "            self._log_feature_importance(model, rec_type)\n",
    "        \n",
    "        return self.training_metrics\n",
    "    \n",
    "    def _tune_model(self, base_model, X_train, y_train, rec_type: str):\n",
    "        \"\"\"Tune model hyperparameters using grid search.\n",
    "        \n",
    "        Args:\n",
    "            base_model: Base model instance\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            rec_type: Recommendation type name\n",
    "            \n",
    "        Returns:\n",
    "            Best model from grid search\n",
    "        \"\"\"\n",
    "        logger.info(f\"Performing hyperparameter tuning...\")\n",
    "        \n",
    "        param_grid = self._get_param_grid()\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=self.config.cv_folds,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        logger.info(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        logger.info(f\"Best CV F1 score: {grid_search.best_score_:.3f}\")\n",
    "        \n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    def _evaluate_model(self, model, X_train, X_test, y_train, y_test, \n",
    "                        rec_type: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate trained model and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            X_train: Training features\n",
    "            X_test: Test features\n",
    "            y_train: Training targets\n",
    "            y_test: Test targets\n",
    "            rec_type: Recommendation type name\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        # Predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=self.config.cv_folds,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
    "            'f1_score': report['weighted avg']['f1-score'],\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        logger.info(f\"\\nMetrics for {rec_type}:\")\n",
    "        logger.info(f\"  Train Accuracy: {metrics['train_accuracy']:.3f}\")\n",
    "        logger.info(f\"  Test Accuracy:  {metrics['test_accuracy']:.3f}\")\n",
    "        logger.info(f\"  F1 Score:       {metrics['f1_score']:.3f}\")\n",
    "        logger.info(f\"  Precision:      {metrics['precision']:.3f}\")\n",
    "        logger.info(f\"  Recall:         {metrics['recall']:.3f}\")\n",
    "        logger.info(f\"  CV Score:       {metrics['cv_mean']:.3f} Â± {metrics['cv_std']:.3f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        accuracy_diff = metrics['train_accuracy'] - metrics['test_accuracy']\n",
    "        if accuracy_diff > 0.15:\n",
    "            logger.warning(f\"âš ï¸  Potential overfitting detected (train-test gap: {accuracy_diff:.3f})\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _log_feature_importance(self, model, rec_type: str, top_n: int = 5):\n",
    "        \"\"\"Log top important features for the model.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            rec_type: Recommendation type name\n",
    "            top_n: Number of top features to display\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'feature_importances_'):\n",
    "            return\n",
    "        \n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:top_n]\n",
    "        \n",
    "        logger.info(f\"\\nTop {top_n} features for {rec_type}:\")\n",
    "        for i, idx in enumerate(indices, 1):\n",
    "            logger.info(f\"  {i}. {self.feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "    \n",
    "    def train_impact_predictor(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"Train a regression model to predict recommendation impact scores.\n",
    "        \n",
    "        This model predicts how much a recommendation will improve the profile.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame (already scaled)\n",
    "            y: Target series with success scores\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with regression metrics\n",
    "        \"\"\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(\"Training Impact Prediction Model\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.config.test_size,\n",
    "            random_state=self.config.random_state\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "        \n",
    "        # Initialize regression model\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            subsample=0.9,\n",
    "            random_state=self.config.random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        logger.info(\"Training regression model...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'train_mse': mean_squared_error(y_train, y_pred_train),\n",
    "            'test_mse': mean_squared_error(y_test, y_pred_test),\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "            'train_r2': r2_score(y_train, y_pred_train),\n",
    "            'test_r2': r2_score(y_test, y_pred_test)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        logger.info(\"\\nImpact Predictor Metrics:\")\n",
    "        logger.info(f\"  Train RMSE: {metrics['train_rmse']:.4f}\")\n",
    "        logger.info(f\"  Test RMSE:  {metrics['test_rmse']:.4f}\")\n",
    "        logger.info(f\"  Train RÂ²:   {metrics['train_r2']:.4f}\")\n",
    "        logger.info(f\"  Test RÂ²:    {metrics['test_r2']:.4f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        r2_diff = metrics['train_r2'] - metrics['test_r2']\n",
    "        if r2_diff > 0.15:\n",
    "            logger.warning(f\"âš ï¸  Potential overfitting detected (train-test RÂ² gap: {r2_diff:.3f})\")\n",
    "        \n",
    "        # Store model\n",
    "        self.models['impact_predictor'] = model\n",
    "        self.training_metrics['impact_predictor'] = metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save_model(self) -> Path:\n",
    "        \"\"\"Save trained models and metadata to disk.\n",
    "        \n",
    "        Returns:\n",
    "            Path to saved model file\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If no models have been trained\n",
    "        \"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models to save. Train models first.\")\n",
    "        \n",
    "        output_path = Path(self.config.output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        model_name = f'github_optimizer_{self.config.model_type}_{timestamp}.pkl'\n",
    "        \n",
    "        # Prepare model data\n",
    "        model_data = {\n",
    "            'models': self.models,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names,\n",
    "            'recommendation_types': GitHubProfileDataset.TARGET_COLUMNS,\n",
    "            'model_type': self.config.model_type,\n",
    "            'training_config': asdict(self.config),\n",
    "            'training_metrics': self.training_metrics,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        # Save model\n",
    "        model_file = output_path / model_name\n",
    "        joblib.dump(model_data, model_file, compress=3)\n",
    "        logger.info(f\"âœ“ Model saved to: {model_file}\")\n",
    "        \n",
    "        # Save human-readable metadata\n",
    "        metadata = {\n",
    "            'model_file': model_name,\n",
    "            'timestamp': timestamp,\n",
    "            'model_type': self.config.model_type,\n",
    "            'feature_names': self.feature_names,\n",
    "            'recommendation_types': GitHubProfileDataset.TARGET_COLUMNS,\n",
    "            'training_config': asdict(self.config),\n",
    "            'training_metrics': self.training_metrics\n",
    "        }\n",
    "        \n",
    "        metadata_file = output_path / f'metadata_{timestamp}.json'\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"âœ“ Metadata saved to: {metadata_file}\")\n",
    "        \n",
    "        return model_file\n",
    "    \n",
    "    def predict_recommendations(self, X: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Predict recommendations for new profiles.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame (should be scaled using the same scaler)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping recommendation types to probability arrays\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If models haven't been trained\n",
    "        \"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models available. Train or load models first.\")\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        for rec_type in GitHubProfileDataset.TARGET_COLUMNS:\n",
    "            if rec_type not in self.models:\n",
    "                logger.warning(f\"Model for {rec_type} not found, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            model = self.models[rec_type]\n",
    "            # Get probability of positive class (needs recommendation)\n",
    "            predictions[rec_type] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461d34a",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "\n",
    "Now let's run the training pipeline with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8397e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = TrainingConfig(\n",
    "    n_samples=2000,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    cv_folds=5,\n",
    "    tune_hyperparameters=True,\n",
    "    model_type='random_forest',\n",
    "    output_dir='models'\n",
    ")\n",
    "\n",
    "# Validate configuration\n",
    "config.validate()\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"GitHub Profile Optimizer - Model Training Pipeline\")\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(f\"Configuration:\")\n",
    "logger.info(f\"  Samples: {config.n_samples}\")\n",
    "logger.info(f\"  Model Type: {config.model_type}\")\n",
    "logger.info(f\"  Hyperparameter Tuning: {config.tune_hyperparameters}\")\n",
    "logger.info(f\"  Test Size: {config.test_size}\")\n",
    "logger.info(f\"  CV Folds: {config.cv_folds}\")\n",
    "logger.info(f\"  Output Directory: {config.output_dir}\")\n",
    "logger.info(f\"  Random Seed: {config.random_state}\")\n",
    "logger.info(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068cc449",
   "metadata": {},
   "source": [
    "# Step 1: Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f660a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"STEP 1: Preparing Dataset\")\n",
    "logger.info(\"-\" * 70)\n",
    "dataset = GitHubProfileDataset(config)\n",
    "df = dataset.generate_synthetic_data()\n",
    "logger.info(f\"âœ“ Dataset ready: {df.shape[0]} samples, {df.shape[1]} columns\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42267940",
   "metadata": {},
   "source": [
    "# Step 2: Preprocess features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"STEP 2: Preprocessing Features\")\n",
    "logger.info(\"-\" * 70)\n",
    "X, scaler = dataset.preprocess_features(df)\n",
    "logger.info(\"âœ“ Features preprocessed and scaled\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723d969",
   "metadata": {},
   "source": [
    "# Step 3: Prepare targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6027a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"STEP 3: Preparing Target Variables\")\n",
    "logger.info(\"-\" * 70)\n",
    "y_classification = df[GitHubProfileDataset.TARGET_COLUMNS]\n",
    "y_regression = df['profile_success_score']\n",
    "\n",
    "logger.info(f\"Classification targets: {len(GitHubProfileDataset.TARGET_COLUMNS)}\")\n",
    "logger.info(f\"Regression target: profile_success_score\")\n",
    "logger.info(f\"âœ“ Targets prepared\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d94122",
   "metadata": {},
   "source": [
    "# Step 4: Train recommendation classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"STEP 4: Training Recommendation Classifiers\")\n",
    "logger.info(\"-\" * 70)\n",
    "model = RecommendationModel(config)\n",
    "model.scaler = scaler\n",
    "\n",
    "classification_metrics = model.train(X, y_classification)\n",
    "logger.info(\"âœ“ Classification models trained\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce1163",
   "metadata": {},
   "source": [
    "# Step 5: Train impact predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe089a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"STEP 5: Training Impact Predictor\")\n",
    "logger.info(\"-\" * 70)\n",
    "impact_metrics = model.train_impact_predictor(X, y_regression)\n",
    "logger.info(\"âœ“ Impact predictor trained\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45f3f4",
   "metadata": {},
   "source": [
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"STEP 6: Saving Models\")\n",
    "logger.info(\"-\" * 70)\n",
    "model_path = model.save_model()\n",
    "logger.info(\"âœ“ All models saved successfully\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate training summary\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(\"TRAINING COMPLETE - SUMMARY\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "logger.info(\"\\nðŸ“Š Classification Model Performance:\")\n",
    "logger.info(\"-\" * 70)\n",
    "for rec_type, metrics in classification_metrics.items():\n",
    "    logger.info(f\"\\n{rec_type}:\")\n",
    "    logger.info(f\"  Test Accuracy:  {metrics['test_accuracy']:.3f}\")\n",
    "    logger.info(f\"  F1 Score:       {metrics['f1_score']:.3f}\")\n",
    "    logger.info(f\"  Precision:      {metrics['precision']:.3f}\")\n",
    "    logger.info(f\"  Recall:         {metrics['recall']:.3f}\")\n",
    "    logger.info(f\"  CV Score:       {metrics['cv_mean']:.3f} Â± {metrics['cv_std']:.3f}\")\n",
    "\n",
    "logger.info(\"\\nðŸ“ˆ Impact Predictor Performance:\")\n",
    "logger.info(\"-\" * 70)\n",
    "logger.info(f\"  Test RMSE:  {impact_metrics['test_rmse']:.4f}\")\n",
    "logger.info(f\"  Test RÂ²:    {impact_metrics['test_r2']:.4f}\")\n",
    "\n",
    "logger.info(\"\\nðŸ’¾ Saved Artifacts:\")\n",
    "logger.info(\"-\" * 70)\n",
    "logger.info(f\"  Model File: {model_path}\")\n",
    "logger.info(f\"  Log File:   training.log\")\n",
    "\n",
    "logger.info(\"\\nðŸš€ Next Steps:\")\n",
    "logger.info(\"-\" * 70)\n",
    "logger.info(\"1. Review training.log for detailed training information\")\n",
    "logger.info(\"2. Check metadata JSON for model specifications\")\n",
    "logger.info(\"3. Integrate model into predictive_optimizer.py:\")\n",
    "logger.info(f\"   optimizer = PredictiveOptimizer(model_path='{model_path}')\")\n",
    "logger.info(\"4. Test model with real GitHub profiles\")\n",
    "logger.info(\"5. Monitor model performance and retrain if needed\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"âœ… Training pipeline completed successfully!\")\n",
    "logger.info(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327ed9e",
   "metadata": {},
   "source": [
    "## Using the Trained Model\n",
    "\n",
    "After training, you can use the model with the predictive optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de85731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the trained model\n",
    "from src.github_profile_optimizer.ml.predictive_optimizer import PredictiveOptimizer\n",
    "\n",
    "# Initialize with the trained model path\n",
    "optimizer = PredictiveOptimizer(model_path=str(model_path))\n",
    "\n",
    "# The optimizer will now use the trained ML models for recommendations\n",
    "print(\"Predictive optimizer initialized with trained model!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
